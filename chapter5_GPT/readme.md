# 5장

가장 간단한 텍스트 생성 task를 실행함

기본적인 디코딩을 배우고 주로 샘플링 기법을 공부함  
 

## 디코딩  
디코딩에는 
1.그리디 서치 디코딩: 말 그대로 그리디를 사용해 최고 확률의 토큰을 사용함 구현이 간단하고 실행이 빠름  
> $\hat{y}=\text{argmax}_y \ P(y|x)$     

하지만 같은 단어를 계속 사용한다는 단점이 있어서 문장의 다양성이 확실히 떨어짐  
2.빔 서치 디코딩: 제일 무난하게 사용하는 디코딩, 확률 높은거 b개를 골라서 로그 확률을 계산해 높은 것을 고르는 것을 반복 안정적임  
> 아니 이거 latex 왜 오류 뜨는거임?? 몇번을 해도 Unable to render expression.뜨면서 해결이 안됨 ipynb 내부 보고 공부하기

하지만 b가 높으면 성능은 좋지만 속도가 굉장히 떨어짐  

## 샘플링  
샘플링에는  
1.탑-K 샘플링: 모델의 출력 시퀀스 토큰 확률을 내림차 순으로 만들고 탑 K개의 토큰만 사용하여 랜덤 샘플링 진행함  
하지만 고정적인 컷오프이므로 안 좋을 때가 있다고 함
2. 뉴클리어스 샘플링 (탑-P 샘플링): cdf를 만들어서 확률 질량이 p가 되는 지점까지의 토큰만을 사용함 동적이라 안정적일듯?

샘플링에 정답은 없음 많이 해보고 실험과 주관적인 평가가 들어감
