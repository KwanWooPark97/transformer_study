# 8장
앞에서 사용한 트랜스포머 모델들에서 정확도 만으로는 성능이 좋다는 것을 확신할수없다.  
모델이 너무 느리거나 크다면 성능이 최고여도 의미없다.  
그 대안은 빠르고 작은 모델을 훈련하는 것이다. 하지만 모델이 작아지면 종종 성능이 저하된다.  
이 장에서는 예측 속도를 높이고 트랜스포머 모델의 메모리 사용량을 줄이는 기술 4가지를 공부한다.  

1. 지식 정제(knowledge distillation)
2. 양자화(quantization)
3. 가지치기(pruning)
4. ONNX(Open Neural Network Exchange)포맷과 ONNX 런타임을 사용한 그래프 최적화

## 지식 정제로 모델 크기 줄이기
내가 알고있는 teacher-student 방식으로 이해 함.
느리고 크지만 성능 좋은 선생 모델을 가져와 작은 학생 모델을 만들어서 선생 모델을
모방 학습 하도록 진행
사전 훈련하는 언어 모델의 파라미터 개수가 꾸준히 증가하는 경향을 고려할 때,
지식 정제는 대규모 모델을 압축해 실용적으로 만들 수 있다.  

## 양자화
계산량을 줄이는 대신, 가중치와 활성화 출력을 32비트 부동 소수점이 아닌 8비트 정수같이
정밀도가 낮은 데이터 타입으로 표현해 계산을 더 효율적으로 수행한다.
비트 수를 줄이면 모델에 필요한 메모리 양이 줄고 행렬 곱셈 같은 연산이
정수 계산으로 훨씬 빠르게 수행된다. 근데 성능에는 영향이 거의 없다.

## ONNX와 ONNX 런타임으로 추론 최적화하기
ONNX 포맷으로 내보내면 이런 연산자를 사용해 신경망을 통과하는 데이터 흐름을 표현하기 위해
계산 그래프를 만든다. 밑에 BERT의 그림을 보여줌
여기서 노드는 어떤 입력을 받고 Add나 Squeeze같은 연산을 적용하고 그다음 출력을 다음 노드로 전달한다.  

## 가중치 가중치기
메모리 사용이 극단적으로 제한되는 상황에서 최대한 메모리 사용을 줄이기 위해
제일 쓸모없는 가중치를 지워버리면서 파라미터 줄이기  

